---
title: "Clustering: A pipeline to analyse time-course gene expression data"
output: rmarkdown::html_vignette
bibliography: bibliography.bib
---


```{r echo=FALSE, results="hide", message=FALSE}
# Loading dependencies
library(devtools)
library(limma)
library(splines)
library(stats)
library(moanin)
library(NMF)
source("utils.R")
# library(moanin)
```

```{r echo=FALSE, results="hide"}
# Set options
df = 6

# Preprocessing & filtering
is_count = FALSE
take_log = FALSE    
filter_expression = FALSE

###############################################################################
# Clustering

# Whether to filter genes
filter_genes = TRUE
keep_all_significant = FALSE
n_genes_to_keep = 5000
n_clusters = 20
percentage_genes_to_label = 0.5
```

```{r echo=FALSE, results="hide"}
# Set data files and options
data("shoemaker2015")

data = shoemaker2015$data
meta = shoemaker2015$meta

data = read.table(".results/quantile_normalized.txt") 

# Right now, the package doesn't contain the normalized data
de_analysis = read.table(".results/pvalues.txt", sep="\t", check.names=FALSE)

# Extract pvalues and log_fold change
lfc_col_to_keep = colnames(de_analysis)[
    grepl("-lfc_max", colnames(de_analysis))]
lfc_max = de_analysis[, lfc_col_to_keep]

pval_col_to_keep = colnames(de_analysis)[
    grepl("-pval", colnames(de_analysis))]
pvalues = de_analysis[, pval_col_to_keep]

splines_model = moanin::create_splines_model(meta, degrees_of_freedom=6)
```

# Clustering of time-course data

The next step is to perform clustering. First, we reduce the set of genes of
interest to genes that are (1) significant; (2) "highly" differentially
expressed. First, we select the genes that have a log-fold change of at least
2 across at least one experiment for one time point. We thus use the object
`lfc_max`. 

XXX


```{r filter_genes}
if(filter_genes){
    # First, filter out any genes that doesn't have a log fold change of at
    # least 2 at at least one of the time points.
    
    genes_to_keep = row.names(lfc_max[rowSums(lfc_max > 2) > 0, ])
    pvalues = pvalues[genes_to_keep, ]

    if(length(genes_to_keep) > n_genes_to_keep){

	# Then rank by fisher's p-value and take max the number of genes of
	# interest
	# Filter out q-values for the pvalues table
	fishers_pval = moanin:::fisher_method(pvalues)
	fishers_qval = stats::p.adjust(fishers_pval)
        if(keep_all_significant){
	   genes_to_keep = names(fishers_qval[fishers_qval < 0.05])
	}else{
	    genes_to_keep = names(sort(fishers_pval)[1:n_genes_to_keep])
	}
    }

    y = as.matrix(data[genes_to_keep, ])
}else{
    y = as.matrix(data)
}
```

After filtering, we consider `r dim(y)[1]` genes for the clustering.

```{r clustering}

#############################################################################
# Fit the splines

kmeans_clusters = moanin:::splines_kmeans(
    y, splines_model, n_clusters=20,
    random_seed=42,
    n_init=20)

# Rescale the centroids
centroids = moanin:::rescale_values(kmeans_clusters$centroids)


# XXX is there a bug here?
# all_scores = data.frame(row.names=row.names(data))
# for(k in 1:n_clusters){
#     scores = moanin:::score_genes_centroid(data, centroids[k,])
#     all_scores[paste0("cluster_", k)] = scores
# }

# Only assign labels to X% of the genes
# max_score = quantile(moanin:::row_min(all_scores), c(percentage_genes_to_label))
# genes_to_not_consider = moanin:::row_min(all_scores) >= max_score
# labels = moanin:::row_argmin(all_scores)
# labels[genes_to_not_consider] = NA
```

```{r}
table(kmeans_clusters$clusters)
```

```{r fig.height=8, fig.width=8}
moanin:::plot_centroids(centroids, splines_model,
			colors=ann_colors$Group,
			smooth=TRUE)
```

Now, let's look at genes in cluster 15

```{r fig.width=8, fig.height=8}
gene_names = row.names(data)
genes_to_plot =  names(kmeans_clusters$clusters[kmeans_clusters$clusters == 15])
data_to_plot = data[genes_to_plot, ]
submeta = meta
submeta$Group = factor(submeta$Group, levels(submeta$Group)[c(3, 1, 2, 5, 4)])
ord = order(
  submeta$Group,
  submeta$Timepoint,
  submeta$Replicate)
submeta$Timepoint = as.factor(submeta$Timepoint)

data_to_plot = data_to_plot[, ord]
submeta = submeta[ord, ]

NMF::aheatmap(
    data_to_plot,
    Colv=NA,
    annCol=submeta[,
                c("Group", "Timepoint")],
    annLegend=FALSE,
    annColors=ann_colors,
    main="Cluster 15",
    treeheight=0)

NMF::aheatmap(
    moanin:::rescale_values(data_to_plot),
    Colv=NA,
    annCol=submeta[,
                c("Group", "Timepoint")],
    annLegend=FALSE,
    annColors=ann_colors,
    main="Cluster 15",
    treeheight=0)

```

## How to choose the number of clusters.

A common question that arises when performing clustering is how to choose the
number of clusters. A choice for the number of clusters K depends on the goal. 
In this particular case, the end goal is not the clustering, but to facilitate
interpretation of the differential expression analysis step. As a result, the
number of clusters should not exceed the number of gene sets the user wants to
interpret. This allows to set a maximum number of clusters. Let us assume here
that this number is 20 clusters.

Once the maximum number of clusters is set, several strategies allow to
identify the number of clusters:

    - Elbow method
    - *Silhouette method* 
    - *Model explorer*

```{r running_clustering_on_all_k}
all_possible_n_clusters = c(5, 7, 9, 11)
all_clustering = list()
wss_values = list()

i = 1
for(n_cluster in all_possible_n_clusters){
    clustering_results = moanin:::splines_kmeans(
	y, splines_model,
	n_clusters=n_cluster, random_seed=42,
	n_init=10)
    wss_values[i] = sum(clustering_results$WCSS_per_cluster)
    all_clustering[[i]] = clustering_results$clusters
    i = i + 1
}
```

### Elbow method

```{r elbow_method, fig.width=6, fig.height=4}

# function to compute total within-cluster sum of square 

plot(all_possible_n_clusters, wss_values,
     type="b", pch=19, frame=FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

```

### Average silhouette method

```{r average_silhouette_method, fig.width=6, fig.height=4}
# function to compute average silhouette for k clusters
average_silhouette = function(labels, y) {
    silhouette_results = cluster::silhouette(unlist(labels[1]), dist(y))
    return(mean(silhouette_results[, 3]))
}

# extract the average silhouette
average_silhouette_values = list()
i = 1
for(i in 1:length(all_clustering)){
    labels = all_clustering[i]
    average_silhouette_values[i] = average_silhouette(labels, y)
    i = i + 1
}

plot(all_possible_n_clusters, average_silhouette_values,
     type="b", pch=19, frame=FALSE,
     xlab="Number of clusters K",
     ylab="Average Silhouettes")
```

### Looking at the stability of the clustering

The last strategy is to look at the stability of the clustering.

```{r fig.width=6, fig.height=4}
library(NMF)
stability_5 = read.table("results/stability_5.tsv", sep="\t")
consensus_matrix_stability_5 = moanin::consensus_matrix(stability_5,
							scale=FALSE)
NMF::aheatmap(consensus_matrix_stability_5[1:1000, 1:1000], Rowv=FALSE,
	      Colv=FALSE,
	      treeheight=0)

stability_20 = read.table("results/stability_15.tsv", sep="\t")
consensus_matrix_stability_20 = moanin::consensus_matrix(stability_20,
							 scale=FALSE)
NMF::aheatmap(consensus_matrix_stability_20[1:1000, 1:1000], Rowv=FALSE,
	      Colv=FALSE,
	      treeheight=0)

```


```{r}
n_clusters = 5:20
all_labels = list()
for(i in n_clusters){
    filename = paste0("results/stability_", i, ".tsv")
    stability = read.table(filename, sep="\t")
    all_labels[[paste0("C", i)]] = stability
}
```

```{r fig.width=8, fig.height=8}
library(moanin)
moanin::plot_model_explorer(all_labels)
```


```{r results="hide", echo=FALSE}
# Save the clustering for the next step
kmeans_labels = as.data.frame(kmeans_clusters$clusters)
row.names(kmeans_labels) = row.names(y)
write.table(kmeans_labels,
	    ".results/clustering_labels.txt",
	    sep="\t")
```
