---
title: "Clustering: A pipeline to analyse time-course gene expression data"
output: rmarkdown::html_vignette
bibliography: bibliography.bib
---


```{r echo=FALSE, results="hide", message=FALSE}
# Loading dependencies
library(devtools)
library(limma)
library(splines)
library(stats)
library(moanin)
library(NMF)
source("utils.R")
# library(moanin)
```

```{r echo=FALSE, results="hide"}
# Set options
df = 6

# Preprocessing & filtering
is_count = FALSE
take_log = FALSE    
filter_expression = FALSE

###############################################################################
# Clustering

# Whether to filter genes
filter_genes = TRUE
keep_all_significant = FALSE
n_genes_to_keep = 10000
n_clusters = 20
percentage_genes_to_label = 0.5
```

```{r echo=FALSE, results="hide"}
# Set data files and options
data("shoemaker2015")

data = shoemaker2015$data
meta = shoemaker2015$meta

data = read.table(".results/quantile_normalized.txt") 

# Right now, the package doesn't contain the normalized data
de_analysis = read.table(".results/pvalues.txt", sep="\t", check.names=FALSE)

# Extract pvalues and log_fold change
lfc_col_to_keep = colnames(de_analysis)[
    grepl("-lfc_max", colnames(de_analysis))]
lfc_max = de_analysis[, lfc_col_to_keep]

pval_col_to_keep = colnames(de_analysis)[
    grepl("-pval", colnames(de_analysis))]
pvalues = de_analysis[, pval_col_to_keep]

splines_model = moanin::create_splines_model(meta, degrees_of_freedom=6)
```

# Clustering of time-course data

The next step is to perform clustering. First, we reduce the set of genes of
interest to genes that are (1) significant; (2) "highly" differentially
expressed. First, we select the genes that have a log-fold change of at least
2 across at least one experiment for one time point. 

```{r filter_genes}
# First, filter out any genes that doesn't have a log fold change of at
# least 2 at at least one of the time points.

genes_to_keep = row.names(lfc_max[rowSums(lfc_max > 2) > 0, ])
pvalues = pvalues[genes_to_keep, ]

# Then rank by fisher's p-value and take max the number of genes of interest
# Filter out q-values for the pvalues table
fishers_pval = moanin:::fisher_method(pvalues)
fishers_qval = stats::p.adjust(fishers_pval)
genes_to_keep = names(sort(fishers_pval))
if(length(genes_to_keep) > n_genes_to_keep){
    genes_to_keep = genes_to_keep[1:n_genes_to_keep]
}

y = as.matrix(data[genes_to_keep, ])
```

After filtering, we consider `r dim(y)[1]` genes for the clustering.

```{r clustering}
# First fit the kmeans clusters
kmeans_clusters = moanin:::splines_kmeans(
    y, splines_model, n_clusters=20,
    random_seed=42,
    n_init=20)

#Â Then assign scores and labels to all the data, using a goodness-of-fit
# scoring function.
scores_and_labels = moanin::splines_kmeans_score_and_label(
    data, kmeans_clusters)
labels = scores_and_labels$labels
```

Summary of number of genes assigned with original scoring, and "better"
scoring.

```{r}
barplot(table(kmeans_clusters$clusters), col="black")
```

```{r}
barplot(table(labels), col="black")
```

Just a hack, cause I didn't update the code
```{r}
kmeans_clusters$clusters = labels[!is.na(labels)]
```

```{r results="hide", echo=FALSE}
# Save the clustering for the next step
kmeans_labels = as.data.frame(labels[!is.na(labels)])
write.table(kmeans_labels,
	    ".results/clustering_labels.txt",
	    sep="\t")
```

Now let's visualize the centroids.

```{r fig.height=8, fig.width=8}
moanin:::plot_centroids(kmeans_clusters$centroids, splines_model,
			colors=ann_colors$Group,
			smooth=TRUE)
```

Now, let's look at genes in cluster 15

```{r fig.width=8, fig.height=8}
cluster_to_plot = 13
gene_names = row.names(data)
genes_to_plot =  names(kmeans_clusters$clusters[kmeans_clusters$clusters == cluster_to_plot])
data_to_plot = data[genes_to_plot, ]
submeta = meta
submeta$Group = factor(submeta$Group, levels(submeta$Group)[c(3, 1, 2, 5, 4)])
ord = order(
  submeta$Group,
  submeta$Timepoint,
  submeta$Replicate)
submeta$Timepoint = as.factor(submeta$Timepoint)

data_to_plot = data_to_plot[, ord]
submeta = submeta[ord, ]

NMF::aheatmap(
    data_to_plot,
    Colv=NA,
    annCol=submeta[,
                c("Group", "Timepoint")],
    annLegend=FALSE,
    annColors=ann_colors,
    main="Cluster 15",
    treeheight=0)

NMF::aheatmap(
    moanin:::rescale_values(data_to_plot),
    Colv=NA,
    annCol=submeta[,
                c("Group", "Timepoint")],
    annLegend=FALSE,
    annColors=ann_colors,
    main=paste("Cluster", cluster_to_plot),
    treeheight=0)

```

## How to choose the number of clusters.

A common question that arises when performing clustering is how to choose the
number of clusters. A choice for the number of clusters K depends on the goal. 
In this particular case, the end goal is not the clustering, but to facilitate
interpretation of the differential expression analysis step. As a result, the
number of clusters should not exceed the number of gene sets the user wants to
interpret. This allows to set a maximum number of clusters. Let us assume here
that this number is 20 clusters.

Once the maximum number of clusters is set, several strategies allow to
identify the number of clusters:

    - Elbow method
    - *Silhouette method* 
    - *Model explorer*

```{r running_clustering_on_all_k}
all_possible_n_clusters = c(5, 7, 9, 11)
all_clustering = list()
wss_values = list()

i = 1
for(n_cluster in all_possible_n_clusters){
    clustering_results = moanin:::splines_kmeans(
	y, splines_model,
	n_clusters=n_cluster, random_seed=42,
	n_init=10)
    wss_values[i] = sum(clustering_results$WCSS_per_cluster)
    all_clustering[[i]] = clustering_results$clusters
    i = i + 1
}
```

### Elbow method

```{r elbow_method, fig.width=6, fig.height=4}

# function to compute total within-cluster sum of square 

plot(all_possible_n_clusters, wss_values,
     type="b", pch=19, frame=FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

```

### Average silhouette method

```{r average_silhouette_method, fig.width=6, fig.height=4}
# function to compute average silhouette for k clusters
average_silhouette = function(labels, y) {
    silhouette_results = cluster::silhouette(unlist(labels[1]), dist(y))
    return(mean(silhouette_results[, 3]))
}

# extract the average silhouette
average_silhouette_values = list()
i = 1
for(i in 1:length(all_clustering)){
    labels = all_clustering[i]
    average_silhouette_values[i] = average_silhouette(labels, y)
    i = i + 1
}

plot(all_possible_n_clusters, average_silhouette_values,
     type="b", pch=19, frame=FALSE,
     xlab="Number of clusters K",
     ylab="Average Silhouettes")
```

### Looking at the stability of the clustering

On real data, the number of clusters is not only unknown but also 
ambiguous: it will depend on the desired clustering resolution of the user.
Yet, in the case of biological data, stability and reproducibility of the
results is necessary to ensure that the biological interpretation of the
results hold when the data or the model is exposed to reasonable
perturbations.

Methods that rely on the stability of the clustering results to choose $k$
thus ensure that the biological interpretation of the clusters hold with
perturbtation to the data. In addition, simulation where the data is generated
with a well defined $k$ show that the clustering is more stable for the
correct of number of the clusters.

Most methods method to find the number of clusters with stability measures
only provide visual aids to guide the user. The first element often visualized
is the consensus matrix: the consensus matrix is an $n \times n$ matrix that
stores the proportion of clustering in which two items are clustered together.
A perfect consensus matrix ordered such as each elements that belong to the
same cluster are adjacent to one another which show blocks along the diagonal
close to 1.

To perform such analysis, the first step is run the clustering several times
on a resampled dataset--either using bootstrap or subsampling.

Using the bootstrapping strategy:
```{r}
n_genes = dim(y)[1]
indices = sample(1:dim(y)[1], n_genes, replace=TRUE)

bootstrapped_y = y[indices, ]
```

Using the subsampling strategy, keeping 80% of the genes:
```{r}
subsample_proportion = 0.8
indices = sample(1:dim(y)[1], n_genes * subsample_proportion, replace=TRUE)
subsampled_y = y[indices, ]
```

Here we plot to stability matrix of the top 1000 genes for $k=5$ and $k=20$.

```{r fig.width=6, fig.height=4}
library(NMF)
stability_5 = read.table("results/stability_5.tsv", sep="\t")
consensus_matrix_stability_5 = moanin::consensus_matrix(stability_5,
							scale=FALSE)
NMF::aheatmap(consensus_matrix_stability_5[1:1000, 1:1000], Rowv=FALSE,
	      Colv=FALSE,
	      treeheight=0)

stability_20 = read.table("results/stability_20.tsv", sep="\t")
consensus_matrix_stability_20 = moanin::consensus_matrix(stability_20,
							 scale=FALSE)
NMF::aheatmap(consensus_matrix_stability_20[1:1000, 1:1000], Rowv=FALSE,
	      Colv=FALSE,
	      treeheight=0)

```



```{r echo=FALSE, results="hide"}
n_clusters = 2:20
all_labels = list()
for(i in n_clusters){
    filename = paste0("results/stability_", i, ".tsv")
    stability = read.table(filename, sep="\t")
    all_labels[[paste0("C", i)]] = stability
}
```


#### The model explorer strategy

The model explorer algorithm [@ben-hur:stability] proposes to estimate the
number of clusters exploiting the observation that if the number of clusters
is correct, the clustering results are stable to bootstrapping.  The
distribution of similaries between bootstrapped results for each $k$ can thus
be compared for different values of $k$ and guide the user in the choice of
number of clusters.

The model explorer strategy works as follows. First, choose a similarity
measure between two partitions or clusters $S(C_1, C_2)$. Examples are the
normalized mutual information or Fowlkes-Mallows. Then perform $n$ bootstrap
experiments to estimate the cluster centroids, followed by a step of assigning
a label to all data points. Finally, compute the pairwise similarity measure
between all bootstrapped partition, and plot the cumulative density of the
obtained scores.

```{r fig.width=8, fig.height=8}
moanin::plot_model_explorer(all_labels)
```

From this plot, we can deduce that $k=5$ is more stable than $k=3$ and $k=4$,
but not as stable as $k=2$. Similarly, $k=14$ is more stable than $k=11$,
$k=12$, $k=13$. The model explorer strategy, in addition to visualizing the
diversity of the centroids, can thus help assessing an adequate number of
clusters.

#### Consensus clustering as a way to find $k$


The  consensus clustering [@monti:consensus] relies on a similar idea but
instead of looking at the cumulative density of similarity measures of
bootstrapped clustering, the authors suggests plotting the cumulative density
of elements of the consensus matrix. 

```{r eval=FALSE}
moanin::plot_cdf_consensus(all_labels)
```

```{r echo=FALSE}
library(knitr)
knitr::include_graphics("images/clustering_CDF_consensus.png")
```

The stability of the clustering based on the consensus matrix can then be
measured via a single number by looking at the area under the curve: the more
stable the clustering, the closer to 0 or 1 will be the entries of consensus
matrix. The consensus clustering strategy thus suggest at looking at either
the AUC as a function of the number of the clusters or the "improvement" in
the AUC as a function of the number of cluster.

```{r echo=FALSE}
library(knitr)
knitr::include_graphics("images/clustering_AUC_consensus.png")
```

```{r echo=FALSE}
library(knitr)
knitr::include_graphics("images/clustering_delta_AUC_consensus.png")
```

The consensus clustering method suggest that the most stable is $k=2$, which
separates over-expressed genes from under-expressed genes. While it is indeed
a very stable clustering, it does not capture the range of gene expression
patterns present in the data.
